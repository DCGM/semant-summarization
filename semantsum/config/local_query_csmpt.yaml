name: Local Query CSMPT  # Name of the summarization.
summary_type: query-based multi-document # Summarization type.
provider: local # Provider of the summarization.
description: Performs query-based multi-document summarization. # Description of the summarization.
version: 1.0.0 # Version of the summarization workflow.
recommended_num_texts: 10 # Recommended number of texts for summarization.
summarizer: # Summarizer configuration.
  cls: HFQueryBasedMultiDocSummarizer  # name of class that is subclass of Summarizer
  config: # configuration for defined class
    model: BUT-FIT/csmpt-7B-RAGsum  # Name of model that should be used.
    prompt_builder: # Prompt builder. Available fields: text, query. Text is list of strings and query is optional string (None when missing).
      template: |  # Jinja2 template for prompt sequence. It can be a string, dictionary with keys 'segment_name' and 'template', or a sequence of messages with role and content. If you use dictionary all parts will be concatenated and SegmentedString will be used for the rendered result.
        Shrň následující výsledky pro dotaz "{{query}}".
        |Výsledky|: {% for t in text %}Výsledek [{{loop.index}}]: {{t}}
        {% endfor %}|Shrnutí|:
      role_key_form: role # How the dict key would be named in built prompt sequence. Doesn't affect key name in template config.
      content_key_form: content # How the dict key would be named in built prompt sequence. Doesn't affect key name in template config.
    generation_config: # Generation configuration for the model, it overrides the default generation configuration.
      max_new_tokens: 16000 # The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.
      top_p: 0.95 # If set to float < 1, only the smallest set of most probable tokens with probabilities that add up to `top_p` or higher are kept for generation.
      repetition_penalty: 1.0 # The parameter for repetition penalty. 1.0 means no penalty.
      do_sample: true # Whether or not to use sampling ; use greedy decoding otherwise.
      use_cache: true # Whether or not the model should use the past last key/values attentions (if applicable to the model) to speed up decoding.
    cache_dir: # The directory where the cache will be stored.
    quantization: # Configuration for bits and bytes quantization.
      load_in_8bit: false  # This flag is used to enable 8-bit quantization with LLM.int8().
      load_in_4bit: false # This flag is used to enable 4-bit quantization by replacing the Linear layers with FP4/NF4 layers from `bitsandbytes`.
      llm_int8_threshold: 6.0 # This corresponds to the outlier threshold for outlier detection as described in `LLM.int8() : 8-bit Matrix Multiplication for Transformers at Scale` paper: https://arxiv.org/abs/2208.07339 Any hidden states value that is above this threshold will be considered an outlier and the operation on those values will be done in fp16. Values are usually normally distributed, that is, most values are in the range [-3.5, 3.5], but there are some exceptional systematic outliers that are very differently distributed for large models. These outliers are often in the interval [-60, -6] or [6, 60]. Int8 quantization works well for values of magnitude ~5, but beyond that, there is a significant performance penalty. A good default threshold is 6, but a lower threshold might be needed for more unstable models (small models, fine-tuning).
      llm_int8_skip_modules: # An explicit list of the modules that we do not want to convert in 8-bit. This is useful for models such as Jukebox that has several heads in different places and not necessarily at the last position. For example for `CausalLM` models, the last `lm_head` is kept in its original `dtype`.
      llm_int8_enable_fp32_cpu_offload: false # This flag is used for advanced use cases and users that are aware of this feature. If you want to split your model in different parts and run some parts in int8 on GPU and some parts in fp32 on CPU, you can use this flag. This is useful for offloading large models such as `google/flan-t5-xxl`. Note that the int8 operations will not be run on CPU.
      llm_int8_has_fp16_weight: false # This flag runs LLM.int8() with 16-bit main weights. This is useful for fine-tuning as the weights do not have to be converted back and forth for the backward pass.
      bnb_4bit_compute_dtype: # This sets the computational type which might be different than the input type. For example, inputs might be fp32, but computation can be set to bf16 for speedups.
      bnb_4bit_quant_type: fp4 # This sets the quantization data type in the bnb.nn.Linear4Bit layers. Options are FP4 and NF4 data types which are specified by `fp4` or `nf4`.
      bnb_4bit_use_double_quant: false # This flag is used for nested quantization where the quantization constants from the first quantization are quantized again.
      bnb_4bit_quant_storage: # This sets the storage type to pack the quanitzed 4-bit prarams.
    torch_dtype: bfloat16 # Override the default torch.dtype and load the model under a specific dtype
    attn_implementation: # The attention implementation to use in the model (if relevant). Can be any of "eager" (manual implementation of the attention), "sdpa" (using F.scaled_dot_product_attention), or "flash_attention_2" (using Dao-AILab/flash-attention). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual "eager" implementation.
    device_map: auto # Device map for model loading.